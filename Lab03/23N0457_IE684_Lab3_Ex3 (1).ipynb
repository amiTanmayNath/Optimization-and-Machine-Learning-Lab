{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 5.} \\large\\textbf{Exercise 1.}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A0GNwKcEv3u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "Recall that to solve problems of the form $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$, the update rule involved in Newton's method is of the form:\n",
        "\\begin{align}\n",
        "\\mathbf{x}^{k+1} = \\mathbf{x}^{k} - \\eta^k (\\nabla^2 f(\\mathbf{x}^{k}))^{-1} \\nabla f(\\mathbf{x}^{k}).   \n",
        "\\end{align}\n",
        "\n",
        "Now we will discuss a method which avoids explicit computation of the inverse of Hessian matrix at each iteration, but is nearly efficient as the Newton's method. This method will be called BFGS named after the famous applied Mathematicians Broyden, Fletcher, Goldfarb and Shanno.\n",
        "\n",
        "The main idea of BFGS method is to replace the inverse of Hessian matrix $(\\nabla^2 f(\\mathbf{x}^{k}))^{-1}$ in the update rule of Newton's method with a surrogate term $B^k$.\n",
        "\n",
        "Therefore the update rule of BFGS looks as follows:\n",
        "\\begin{align}\n",
        "\\mathbf{x}^{k+1} = \\mathbf{x}^{k} - \\eta^k B^k \\nabla f(\\mathbf{x}^{k})   \n",
        "\\end{align}\n",
        "where $B^k$ is a surrogate for the inverse of Hessian matrix.\n",
        "\n",
        "To find a suitable candidate for $B^k$, we need to consider some favorable characteristics expected from $B^k$:\n",
        "\n",
        "\\begin{align}\n",
        "&B^k \\text{ is symmetric positive definite}.  \\\\\n",
        "&B^k \\text{ does not involve computing Hessian or its inverse and should be computable only from the gradients}.  \\\\\n",
        "&\\text{Replacing  } (\\nabla^2 f(\\mathbf{x}^{k}))^{-1} \\text{ with } B^k \\text{ should not slow down the algorithm too much}. \\\\\n",
        "\\end{align}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L9tVzAnqyb-"
      },
      "source": [
        "To design a suitable $B^k$ we shall consider the quadratic approximation of $f$:\n",
        "\n",
        "\\begin{align}\n",
        "\\tilde{f}(\\mathbf{x}) = f(\\mathbf{x}^{k+1}) + \\left \\langle \\nabla f(\\mathbf{x}^{k+1}), \\mathbf{x}-\\mathbf{x}^{k+1}\\right \\rangle  + \\frac{1}{2} (\\mathbf{x}-\\mathbf{x}^{k+1})^\\top H^{k+1} (\\mathbf{x}-\\mathbf{x}^{k+1}).\n",
        "\\end{align}\n",
        "where $H^{k+1} = \\nabla^2 f({\\mathbf{x}}^{k+1})$.\n",
        "\n",
        "Note that using this quadratic approximation we have the gradient as:\n",
        "\\begin{align}\n",
        "\\nabla \\tilde{f}(\\mathbf{x}) = \\nabla f(\\mathbf{x}^{k+1}) + H^{k+1}(\\mathbf{x}-\\mathbf{x}^{k+1}).\n",
        "\\end{align}\n",
        "\n",
        "In order to assume $\\tilde{f}$ to behave similar to $f$, we expect the following.\n",
        "\n",
        "By plugging in $\\mathbf{x} = \\mathbf{x}^k$ and $\\mathbf{x}=\\mathbf{x}^{k+1}$, we expect the following from the previous gradient equation:\n",
        "\\begin{align}\n",
        "\\nabla \\tilde{f} (\\mathbf{x}^k) = \\nabla f(\\mathbf{x}^k) \\text{ and }\\\\\n",
        "\\nabla \\tilde{f} (\\mathbf{x}^{k+1}) = \\nabla f(\\mathbf{x}^{k+1}).\n",
        "\\end{align}\n",
        "\n",
        "The relation $\\nabla \\tilde{f} (\\mathbf{x}^{k+1}) = \\nabla f(\\mathbf{x}^{k+1})$ directly follows from the gradient relation  $\\nabla \\tilde{f}(\\mathbf{x}) = \\nabla f(\\mathbf{x}^{k+1}) + H^{k+1}(\\mathbf{x}-\\mathbf{x}^{k+1})$.\n",
        "\n",
        "For the gradient relation to satisfy $\\nabla \\tilde{f} (\\mathbf{x}^k) = \\nabla f(\\mathbf{x}^k)$ we need:\n",
        "\\begin{align}\n",
        "\\nabla \\tilde{f} (\\mathbf{x}^k) &= \\nabla f(\\mathbf{x}^{k+1}) + H^{k+1}(\\mathbf{x}^{k}-\\mathbf{x}^{k+1}) = \\nabla f(\\mathbf{x}^k) \\\\\n",
        "\\implies H^{k+1}(\\mathbf{x}^{k}-\\mathbf{x}^{k+1}) &= (\\nabla f(\\mathbf{x}^{k})- \\nabla {f} (\\mathbf{x}^{k+1})) \\\\\n",
        "\\implies H^{k+1}(\\mathbf{x}^{k+1}-\\mathbf{x}^{k}) &= (\\nabla f(\\mathbf{x}^{k+1})- \\nabla {f} (\\mathbf{x}^k)).\n",
        "\\end{align}\n",
        "This previous equality is called the $\\textbf{secant equation}$.\n",
        "\n",
        "From the secant equation we see that inverse of $H^{k+1}$ operates on the difference of gradients $(\\nabla f(\\mathbf{x}^{k+1})- \\nabla {f} (\\mathbf{x}^k))$  to yield the difference of iterates $(\\mathbf{x}^{k+1}-\\mathbf{x}^{k})$.\n",
        "\n",
        "The secant equation can be equivalently and compactly written as:\n",
        "\\begin{align}\n",
        "(H^{k+1})^{-1} \\mathbf{y}^k = \\mathbf{s}^k.\n",
        "\\end{align}\n",
        "where $\\mathbf{y}^k = (\\nabla f(\\mathbf{x}^{k+1})- \\nabla {f} (\\mathbf{x}^k))$ and $\\mathbf{s}^k = (\\mathbf{x}^{k+1}-\\mathbf{x}^{k})$.\n",
        "\n",
        "We shall be considering $(H^{k+1})^{-1}$ as a possible choice for $B^{k+1}$ in the BFGS update rule.\n",
        "\n",
        "Hence we make sure that $(H^{k+1})^{-1}$ is positive definite. This is equivalent to considering:\n",
        "\\begin{align}\n",
        "(\\mathbf{y}^{k})^\\top (H^{k+1})^{-1} \\mathbf{y}^k > 0\n",
        "\\end{align}\n",
        "for any non-zero $\\mathbf{y}^k$ which implies that $(\\mathbf{y}^k)^\\top \\mathbf{s}^k > 0$.\n",
        "\n",
        "\n",
        "Generally solving the secant equation $(H^{k+1})^{-1} \\mathbf{y}^k = \\mathbf{s}^k$ leads to infinitely many solutions for the matrix $(H^{k+1})^{-1}$ since there are $n^2$ unknowns and $n$ equations. Hence to select a suitable $(H^{k+1})^{-1}$ we solve an optimization problem of the form:\n",
        "\n",
        "\\begin{align}\n",
        "\\min_H \\|H-(H^k)^{-1}\\| \\ s.t. \\ H=H^\\top, \\ H\\mathbf{y}^k=\\mathbf{s}^k.\n",
        "\\end{align}\n",
        "By using an appropriate norm in the optimization problem, we can get the following update rule for the matrix $(H^{k+1})^{-1} = (I-\\mu^k \\mathbf{s}^k (\\mathbf{y}^k)^\\top) (H^{k})^{-1} (I-\\mu^k \\mathbf{y}^k (\\mathbf{s}^k)^\\top) + \\mu^k \\mathbf{s}^k (\\mathbf{s}^k)^\\top$\n",
        "\n",
        "where $\\mu^k = \\frac{1}{(\\mathbf{y}^k)^\\top \\mathbf{s}^k}$.\n",
        "\n",
        "By taking $B^k = (H^k)^{-1}$, this update rule can now be written as:\n",
        "\n",
        "$B^{k+1} = (I-\\mu^k \\mathbf{s}^k (\\mathbf{y}^k)^\\top) B^{k} (I-\\mu^k \\mathbf{y}^k (\\mathbf{s}^k)^\\top) + \\mu^k \\mathbf{s}^k (\\mathbf{s}^k)^\\top$\n",
        "\n",
        "where $\\mu^k = \\frac{1}{(\\mathbf{y}^k)^\\top \\mathbf{s}^k}$.\n",
        "\n",
        "As long as $B^k$ is positive definite, the update rule guarantees that $B^{k+1}$ is also positive definite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Meohokl4xP"
      },
      "source": [
        "Hence in Exercises 1 and 2, we shall be implementing BFGS method to solve problems of the form $\\min_{\\mathbf{x}\\in{\\mathbb{R}}^n} f(\\mathbf{x})$, and check its  performance against Newton method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIMl937EXeFK",
        "outputId": "857ca992-c3df-4a08-e2c0-0e87ba0c35ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken to compute inverse of A: 0.00547722900000025\n"
          ]
        }
      ],
      "source": [
        "#Let us now check the time taken for computing the inverse of a matrix A\n",
        "from timeit import default_timer as timer\n",
        "import numpy as np\n",
        "\n",
        "#create a random nxn matrix\n",
        "n = 100\n",
        "B = np.random.rand(n, n)\n",
        "A = np.matmul(B,B.T) #Note: This construction ensures that A is symmetric\n",
        "A = np.add(A, 0.001*np.identity(n)) #this diagonal perturbation helps to make the matrix positive definite\n",
        "\n",
        "start_time = timer()\n",
        "A_inv = np.linalg.inv(A)\n",
        "end_time = timer()\n",
        "print('Time taken to compute inverse of A:',end_time - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYJaACtKMIGH"
      },
      "source": [
        "$ \\huge{1.}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4h3VvVQMbPS"
      },
      "source": [
        "Identity matrix could be a suitable initial choice $B_0$. This is an appropriate choice as it has good convergence properties, especially when the eigenvalues of the Hessian are well-conditioned. \\\n",
        "In general, any diagonal matrix can act as a decent choice for $B_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_luz84W0lNIF"
      },
      "source": [
        "$ \\huge{2.}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcsCIAntMNdb"
      },
      "outputs": [],
      "source": [
        "def evalf(x,n):\n",
        "  #Input: x is a numpy array of size n\n",
        "  assert type(x) is np.ndarray and len(x) == n #do not allow arbitrary argument\n",
        "  val = 0\n",
        "  for i in range(n-1):   # computing value of function\n",
        "    val += 4*(x[i]**2 - x[i+1])**2 + (x[i] - 1)**2\n",
        "  return val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA1zWRO0xMd6"
      },
      "outputs": [],
      "source": [
        "def evalg(x,n):\n",
        "  #Input: x is a numpy array of size n\n",
        "  assert type(x) is np.ndarray and len(x) == n #do not allow arbitrary arguments\n",
        "  grad = [16*(x[0]**2 - x[1])*x[0] + 2*(x[0] -1)]\n",
        "  for i in range(n-2):\n",
        "     grad.append(16*(x[i+1]**2 - x[i+2])*x[i+1] + 2*(x[i+1] -1) - 8*(x[i]**2 - x[i+1]))\n",
        "  grad.append(- 8*(x[n-2]**2 - x[n-1]))\n",
        "  return np.array(grad).reshape((n,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmmGBKyEl2Yy"
      },
      "outputs": [],
      "source": [
        "def compute_steplength_backtracking_scaled_direction(n,x, gradf, direction, alpha_start, rho, gamma):\n",
        "  assert type(x) is np.ndarray and len(x) == n\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == n\n",
        "  assert type(direction) is np.ndarray and len(gradf) == n\n",
        "  assert type(alpha_start) is float and alpha_start>=0.\n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0.\n",
        "\n",
        "  alpha = alpha_start\n",
        "  while evalf(x+alpha*direction,n)>evalf(x,n)+gamma*alpha*np.matmul(gradf.T,direction):\n",
        "    alpha=rho*alpha\n",
        "\n",
        "  return alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4b7jLeLl6u8"
      },
      "outputs": [],
      "source": [
        "BACKTRACKING_LINE_SEARCH = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbtZZcOZl9UF"
      },
      "outputs": [],
      "source": [
        "def BFGS_minimizer(n,start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size n, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray  and len(start_x) == n   #do not allow arbitrary arguments\n",
        "  assert type(tol) is float and tol>=0\n",
        "\n",
        "  x = start_x.reshape((n,1))\n",
        "  g_x = evalg(x,n)\n",
        "  I = np.identity(n)\n",
        "  B_k = I\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "\n",
        "  k=0\n",
        "  while (np.linalg.norm(g_x) > tol):\n",
        "    p_k = -np.matmul(B_k, g_x)\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(n, x, g_x, p_k, alpha_start, rho, gamma)\n",
        "    else:\n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "\n",
        "    x_prev = x\n",
        "    s_k = np.multiply(step_length,p_k)\n",
        "    x = np.add(x, s_k)\n",
        "    y_k = evalg(x,n)-evalg(x_prev,n)\n",
        "\n",
        "    u_k = 1/(np.matmul(y_k.T,s_k))\n",
        "    a_1 = np.subtract(I , u_k*np.matmul(s_k,y_k.T))\n",
        "    a_2 = np.subtract(I , u_k*np.matmul(y_k, s_k.T))\n",
        "    B_k = np.matmul(np.matmul(a_1,B_k),a_2) + u_k*np.matmul(s_k,s_k.T)\n",
        "\n",
        "    k += 1\n",
        "    g_x = evalg(x,n)\n",
        "\n",
        "  return x, evalf(x,n), k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w9SCW-Jm-Xw"
      },
      "source": [
        "$ \\huge{3.}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3Dn7PNUJWb2"
      },
      "outputs": [],
      "source": [
        "my_tol = 1e-3\n",
        "bfgs_time = []\n",
        "x_val = []\n",
        "obj_val = []\n",
        "num_itr = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un48ENIZJamx"
      },
      "outputs": [],
      "source": [
        "n=1000\n",
        "my_start_x = np.array([0 for i in range(n)])\n",
        "strt_time = timer()\n",
        "x_opt ,f_val ,k = BFGS_minimizer(n, my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,0.9,0.5,0.5 )\n",
        "end_time = timer()\n",
        "num_itr.append(k)\n",
        "x_val.append(x_opt)\n",
        "obj_val.append(f_val)\n",
        "bfgs_time.append(end_time - strt_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idwfMLoYJfSO"
      },
      "outputs": [],
      "source": [
        "n=2500\n",
        "my_start_x = np.array([0 for i in range(n)])\n",
        "strt_time = timer()\n",
        "x_opt ,f_val ,k = BFGS_minimizer(n, my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,0.9,0.5,0.5 )\n",
        "end_time = timer()\n",
        "num_itr.append(k)\n",
        "x_val.append(x_opt)\n",
        "obj_val.append(f_val)\n",
        "bfgs_time.append(end_time - strt_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqLf8-eFKpVA"
      },
      "outputs": [],
      "source": [
        "n=5000\n",
        "my_start_x = np.array([0 for i in range(n)])\n",
        "strt_time = timer()\n",
        "x_opt ,f_val ,k = BFGS_minimizer(n, my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,0.9,0.5,0.5 )\n",
        "end_time = timer()\n",
        "num_itr.append(k)\n",
        "x_val.append(x_opt)\n",
        "obj_val.append(f_val)\n",
        "bfgs_time.append(end_time - strt_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NV1h0Ds-QrDY"
      },
      "outputs": [],
      "source": [
        "n=7500\n",
        "my_start_x = np.array([0 for i in range(n)])\n",
        "strt_time = timer()\n",
        "x_opt ,f_val ,k = BFGS_minimizer(n, my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,0.9,0.5,0.5 )\n",
        "end_time = timer()\n",
        "num_itr.append(k)\n",
        "x_val.append(x_opt)\n",
        "obj_val.append(f_val)\n",
        "bfgs_time.append(end_time - strt_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY0RZkLvhzNV"
      },
      "outputs": [],
      "source": [
        "n=10000\n",
        "my_start_x = np.array([0 for i in range(n)])\n",
        "strt_time = timer()\n",
        "x_opt ,f_val ,k = BFGS_minimizer(n, my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,0.9,0.5,0.5 )\n",
        "end_time = timer()\n",
        "num_itr.append(k)\n",
        "x_val.append(x_opt)\n",
        "obj_val.append(f_val)\n",
        "bfgs_time.append(end_time - strt_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TtUyEoom6uy"
      },
      "outputs": [],
      "source": [
        "n_choices = [1000,2500,5000,7500,10000]\n",
        "\n",
        "# for n in n_choices:\n",
        "#   my_start_x = np.array([0 for i in range(n)])\n",
        "#   strt_time = timer()\n",
        "#   x_opt ,f_val ,k = BFGS_minimizer(n, my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,0.9,0.5,0.5 )\n",
        "#   end_time = timer()\n",
        "#   num_itr.append(k)\n",
        "#   x_val.append(x_opt)\n",
        "#   obj_val.append(f_val)\n",
        "#   bfgs_time.append(end_time - strt_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykizp1henNCP"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znwUvBxsqKwn"
      },
      "outputs": [],
      "source": [
        "table_0 = []\n",
        "col = [\"n value\", \"minimizer\", 'objective function value']\n",
        "table_0.append(col)\n",
        "for i in  range(len(n_choices)):\n",
        "    lst = []\n",
        "    lst.append(n_choices[i])\n",
        "    lst.append(x_val[i])\n",
        "    lst.append(obj_val[i])\n",
        "    table_0.append(lst)\n",
        "\n",
        "print(tabulate(table_0, headers = 'firstrow', tablefmt = 'fancy_grid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgjDDfhwqX3w"
      },
      "outputs": [],
      "source": [
        "table = []\n",
        "col = [\"n\", \"Time taken by BFGS method\"]\n",
        "table.append(col)\n",
        "for i in  range(len(n_choices)):\n",
        "    lst = []\n",
        "    lst.append(n_choices[i])\n",
        "    lst.append(bfgs_time[i])\n",
        "    table.append(lst)\n",
        "\n",
        "print(tabulate(table, headers = 'firstrow', tablefmt = 'fancy_grid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eozqcZ83qnGm"
      },
      "source": [
        "$ \\huge{4.}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC1seloOqlJQ"
      },
      "outputs": [],
      "source": [
        "#method to find Hessian matrix\n",
        "def evalh(x,n):\n",
        "  assert type(x) is np.ndarray and len(x) == n #do not allow arbitrary type arguments\n",
        "  hes = np.zeros((n,n))\n",
        "  hes[0][0] = 48*x[0]**2 - 16*x[1] + 2\n",
        "  hes[0][1] = -16*x[0]\n",
        "\n",
        "  for i in range(1,n-1):\n",
        "    hes[i][i] = 8 + 16*(x[i]**2 - x[i+1]) + 32*x[i]**2 +2\n",
        "    hes[i][i+1] = -16*x[i]\n",
        "    hes[i][i-1] = hes[i-1][i] #because it is a symmetric matrix\n",
        "  hes[n-1][n-2] = hes[n-2][n-1]\n",
        "  hes[n-1][n-1] = 8\n",
        "\n",
        "  return hes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqwbinVFq60s"
      },
      "outputs": [],
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        "\n",
        "def find_minimizer_newton(n,start_x, tol, line_search_type,*args):\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == n #do not allow arbitrary arguments\n",
        "  assert type(tol) is float and tol>=0.\n",
        "\n",
        "  x = start_x\n",
        "  g_x = evalg(x,n)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "\n",
        "  k = 0\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    D_k = np.linalg.inv(evalh(x,n))\n",
        "    p_k = -np.matmul(D_k,g_x)\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(n,x,g_x,p_k,alpha_start, rho, gamma) #call the new function you wrote to compute the steplength\n",
        "    else:\n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "\n",
        "    #implement the gradient descent steps here\n",
        "    x = np.add(x, np.multiply(step_length,p_k))\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x,n) #compute gradient at new point\n",
        "\n",
        "  return x ,evalf(x,n),k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AaYNaEnrBDH"
      },
      "source": [
        "$ \\huge{5.}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuGDyDNwq-Zh"
      },
      "outputs": [],
      "source": [
        "n_val = [1000,2500,5000,7500,10000]\n",
        "my_tol = 1e-3\n",
        "new_time = []\n",
        "x_val1 = []\n",
        "obj_val1 = []\n",
        "num_itr1 =[]\n",
        "m=0\n",
        "for n in n_val:\n",
        "  my_start_x = np.array([0 for i in range(n)]).reshape((n,1))\n",
        "  str_time = timer()\n",
        "  x_opt1 ,f_val1 ,k1 = find_minimizer_newton(n, my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,0.9,0.5,0.5 )\n",
        "  e_time = timer()\n",
        "  num_itr1.append(k1)\n",
        "  x_val1.append(x_opt1)\n",
        "  obj_val1.append(f_val1)\n",
        "  new_time.append(e_time - str_time)\n",
        "  print(\"Optimizer: \",x_val1[m])\n",
        "  m=m+1\n",
        "print(\"Objective: \",obj_val1)\n",
        "print(\"Time taken: \",new_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28outTRrQ4m"
      },
      "source": [
        "The execution had to be interrupted as Newton's method was not terminating. A possibility as to why this is happening could be that the method was taking too much time to compute the inverse of the Hessian matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXDqGMztrv7Z"
      },
      "source": [
        "$ \\huge{6.}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLqaSoM7ryYX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(n_val , bfgs_time, label = \"Time taken by BFGS\")\n",
        "plt.xlabel(\"n values\")\n",
        "plt.ylabel(\"Time Taken\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSMcMJquO9Q-"
      },
      "source": [
        "**For the function g(x)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1_z7ID-O8Os"
      },
      "outputs": [],
      "source": [
        "#Let us now check the time taken for computing the inverse of a matrix A\n",
        "from timeit import default_timer as timer\n",
        "import numpy as np\n",
        "\n",
        "#create a random nxn matrix\n",
        "n = 100\n",
        "B = np.random.rand(n, n)\n",
        "A = np.matmul(B,B.T) #Note: This construction ensures that A is symmetric\n",
        "A = np.add(A, 0.001*np.identity(n)) #this diagonal perturbation helps to make the matrix positive definite\n",
        "\n",
        "start_time = timer()\n",
        "A_inv = np.linalg.inv(A)\n",
        "end_time = timer()\n",
        "print('Time taken to compute inverse of A:',end_time - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tjEcUD4P6ci"
      },
      "source": [
        "#1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7iAg4Z_PnIf"
      },
      "outputs": [],
      "source": [
        "def evalf(x,n):\n",
        "  #Input: x is a numpy array of size n\n",
        "  assert type(x) is np.ndarray and len(x) == n #do not allow arbitrary argument\n",
        "  f = 0\n",
        "  for i in range(n-1):   # computing value of function\n",
        "    f = f + (x[i] -1)**2 + ( x[0] - (x[i])**2 )**2\n",
        "  return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGejak17PqcI"
      },
      "outputs": [],
      "source": [
        "def evalg(x,n):\n",
        "  #Input: x is a numpy array of size n\n",
        "  assert type(x) is np.ndarray and len(x) == n #do not allow arbitrary arguments\n",
        "  grad_lst =[2*(x[0]-1) + 2*(x[0] - x[0]**2)*(1-2*x[0])]\n",
        "  if n>=2:\n",
        "   for i in range(1,n):\n",
        "     grad_lst[0] = grad_lst[0] + 2*(x[0] - x[i]**2)\n",
        "  for j in range(1,n):\n",
        "     grad_lst.append(2*(x[j] -1) - 4*x[j]*(x[0] - x[j]**2))\n",
        "  return np.array(grad_lst).reshape((n,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_qHtl9IPtSc"
      },
      "outputs": [],
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x,n, gradf, direction, alpha_start, rho, gamma):\n",
        "  assert type(x) is np.ndarray and len(x) == n\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == n\n",
        "  assert type(direction) is np.ndarray and len(gradf) == n\n",
        "  assert type(alpha_start) is float and alpha_start>=0.\n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0.\n",
        "\n",
        "  alpha = alpha_start\n",
        "  while evalf(x+alpha*direction,n)>evalf(x,n)+gamma*alpha*np.matmul(gradf.T,direction):\n",
        "    alpha=rho*alpha\n",
        "\n",
        "  return alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Oejm7_RPwJu"
      },
      "outputs": [],
      "source": [
        "BACKTRACKING_LINE_SEARCH = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNvZWvFuQAQR"
      },
      "outputs": [],
      "source": [
        "#BFGS method to find the minimizer\n",
        "\n",
        "def find_minimizer_BFGS(start_x,n, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size n, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray  and len(start_x) == n   #do not allow arbitrary arguments\n",
        "  assert type(tol) is float and tol>=0\n",
        "\n",
        "  x = start_x.reshape((n,1))\n",
        "  g_x = evalg(x,n)\n",
        "  B_k = (1/8)*np.identity(n)\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "\n",
        "  k=0\n",
        "  while (np.linalg.norm(g_x) > tol):\n",
        "    p_k = -np.matmul(B_k, g_x)\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x,n, g_x, p_k, alpha_start, rho, gamma)\n",
        "    else:\n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "\n",
        "    x_prev = x\n",
        "    s_k = np.multiply(step_length,p_k)  #s_k = x_k+1 - x_k equivalently s_k = alpha*p_k\n",
        "    x = np.add(x, s_k)\n",
        "    y_k = evalg(x,n)-evalg(x_prev,n)\n",
        "\n",
        "    u_k = 1/(np.matmul(y_k.T,s_k))\n",
        "    a_1 = np.subtract(np.identity(n) , u_k*np.matmul(s_k,y_k.T))\n",
        "    a_2 = np.subtract(np.identity(n) , u_k*np.matmul(y_k, s_k.T))\n",
        "    B_k = np.matmul(np.matmul(a_1,B_k),a_2) + u_k*np.matmul(s_k,s_k.T)\n",
        "\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x,n) #compute gradient at new point\n",
        "\n",
        "  return x, evalf(x,n), k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZKWpvblQIzR"
      },
      "source": [
        "#3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVrrZAaQQBv8"
      },
      "outputs": [],
      "source": [
        "n_val = [1000,2500,5000,7500,10000]\n",
        "my_tol = 1e-3\n",
        "bfgs_time = []\n",
        "x_val = []\n",
        "obj_val = []\n",
        "num_itr = []\n",
        "for n in n_val:\n",
        "  my_start_x = np.array([0 for i in range(n)])\n",
        "  strt_time = timer()\n",
        "  x_opt ,f_val ,k = find_minimizer_BFGS( my_start_x,n,my_tol,BACKTRACKING_LINE_SEARCH,0.9,0.5,0.5 )\n",
        "  end_time = timer()\n",
        "  num_itr.append(k)\n",
        "  x_val.append(x_opt)\n",
        "  obj_val.append(f_val)\n",
        "  bfgs_time.append(end_time - strt_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC5oMdGwQOle"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHsaf5kFQRvx"
      },
      "outputs": [],
      "source": [
        "#print bfgs time for different n\n",
        "table = []\n",
        "col = [\"n value\", \"time taken by bfgs\"]\n",
        "table.append(col)\n",
        "for i in  range(len(n_val)):\n",
        "    lst = []\n",
        "    lst.append(n_val[i])\n",
        "    lst.append(bfgs_time[i])\n",
        "    table.append(lst)\n",
        "\n",
        "print(tabulate(table, headers = 'firstrow', tablefmt = 'fancy_grid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh4A_psxQZav"
      },
      "source": [
        "#4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6beaGNmlQW-U"
      },
      "outputs": [],
      "source": [
        "#method to find hessian matrix\n",
        "def evalh(x,n):\n",
        "  assert type(x) is np.ndarray and len(x)==n  #do not allow arbitrary arguments\n",
        "  hessn = np.zeros((n,n))\n",
        "  hessn[0][0] = 2 -4*(x[0] - x[0]**2) + 2*(1- 2*x[0])**2\n",
        "  if n>=2:\n",
        "    for i in range(1,n):\n",
        "      hessn[0][0] = hessn[0][0] + 2\n",
        "      hessn[0][i] = -4*x[i]\n",
        "\n",
        "  for i in range(1,n):\n",
        "    hessn[i][i] = 2 - 4*(x[0] - x[i]*2) + 8*(x[i]**2)\n",
        "    hessn[i][0] = hessn[0][i] #because it is a symmetric matrix\n",
        "\n",
        "  return hessn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf3XpJDOQm6Z"
      },
      "outputs": [],
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        "\n",
        "def find_minimizer_newton(start_x,n, tol, line_search_type,*args):\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == n #do not allow arbitrary arguments\n",
        "  assert type(tol) is float and tol>=0.\n",
        "\n",
        "  x = start_x\n",
        "  g_x = evalg(x,n)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "\n",
        "  k = 0\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    D_k = np.linalg.inv(evalh(x,n))\n",
        "    p_k = -np.matmul(D_k,g_x)\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x,n,g_x,p_k,alpha_start, rho, gamma) #call the new function you wrote to compute the steplength\n",
        "    else:\n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "\n",
        "    #implement the gradient descent steps here\n",
        "    x = np.add(x, np.multiply(step_length,p_k))\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x,n) #compute gradient at new point\n",
        "\n",
        "  return x ,evalf(x,n),k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHlxzvhoQucw"
      },
      "source": [
        "#5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UAGPH4aQnvR"
      },
      "outputs": [],
      "source": [
        "n_val = [1000,2500,5000,7500,10000]\n",
        "my_tol = 1e-3\n",
        "new_time = []\n",
        "x_val1 = []\n",
        "obj_val1 = []\n",
        "num_itr1 =[]\n",
        "m=0\n",
        "for n in n_val:\n",
        "  my_start_x = np.array([0 for i in range(n)]).reshape((n,1))\n",
        "  str_time = timer()\n",
        "  x_opt1 ,f_val1 ,k1 = find_minimizer_newton(my_start_x,n,my_tol,BACKTRACKING_LINE_SEARCH,0.9,0.5,0.5 )\n",
        "  e_time = timer()\n",
        "  num_itr1.append(k1)\n",
        "  x_val1.append(x_opt1)\n",
        "  obj_val1.append(f_val1)\n",
        "  new_time.append(e_time - str_time)\n",
        "  print(\"Optimizer: \",x_val1[m])\n",
        "  m=m+1\n",
        "print(\"Objective: \",obj_val1)\n",
        "print(\"time: \",new_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74doQpjMQyw2"
      },
      "outputs": [],
      "source": [
        "table1 = []\n",
        "col = [\"n value\", \"time taken by newton's\"]\n",
        "table1.append(col)\n",
        "for i in  range(len(n_val)):\n",
        "    lst1 = []\n",
        "    lst1.append(n_val[i])\n",
        "    lst1.append(new_time[i])\n",
        "    table1.append(lst1)\n",
        "\n",
        "print(tabulate(table1, headers = 'firstrow', tablefmt = 'fancy_grid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFZM2HUvQ4P2"
      },
      "source": [
        "#6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrrO3c1bQ5nL"
      },
      "outputs": [],
      "source": [
        "table2 = []\n",
        "col = [\"n value\", \"time taken by bfgs\",\"time taken by Newton's Method\"]\n",
        "table2.append(col)\n",
        "for i in  range(len(n_val)):\n",
        "    lst2 = []\n",
        "    lst2.append(n_val[i])\n",
        "    lst2.append(bfgs_time[i])\n",
        "    lst2.append(new_time[i])\n",
        "    table2.append(lst2)\n",
        "\n",
        "print(tabulate(table2, headers = 'firstrow', tablefmt = 'fancy_grid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__vP8KWNQ8aD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(n_val , bfgs_time, color = \"cyan\")\n",
        "plt.plot(n_val , new_time, color = \"red\")\n",
        "plt.xlabel(\"n values\")\n",
        "plt.ylabel(\"time taken\")\n",
        "plt.legend([\"bfgs time\", \"newton time\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSvXUbGIRCO2"
      },
      "source": [
        "**Observations:** The graph shows that compared Newton's method, the BFGS method is more efficient in terms of time taken to reach the optimizer. We can also see that as the number of variables increase, the difference in time taken to reach the optimizer by Newton's method and BFGS method increases. \\\n",
        "Thus, we can say that the BFGS method is a more efficient method than the Newton's method."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}