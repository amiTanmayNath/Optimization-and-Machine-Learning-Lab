{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFFiepyy9FkT"
      },
      "source": [
        "#Part 1\n",
        "\n",
        "# Finding the Minimum Values of \\( q_1(x) \\) and \\( q_2(x) \\)\n",
        "\n",
        "## Minimum Value of \\( q_2(x) \\)\n",
        "Given \\( q_2(x) = 2q_1(x_1) + (2q_2 - 1)x_1 + \\frac{3}{2}q_2^2 - 2q_2 \\), we express it in matrix form as \\( q_2(x) = \\frac{1}{2}x^\\top Ax - b_2^\\top x \\).\n",
        "\n",
        "For \\( q_2(x) \\), we have:\n",
        "- \\( A = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix} \\)\n",
        "- \\( b_2 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\)\n",
        "\n",
        "To find the minimizer \\( x^* \\), we calculate \\( x^* = A^{-1}b_2 \\), resulting in \\( x^* = \\left[ \\frac{1}{11}, \\frac{6}{11} \\right] \\).\n",
        "\n",
        "Substituting \\( x^* \\) into \\( q_2(x) \\), we determine the minimum value of \\( q_2(x) \\) to be \\( \\frac{57}{121} \\).\n",
        "\n",
        "## Minimum Value of \\( q_1(x) \\)\n",
        "Given \\( q_1(x) = \\frac{1}{2}x^\\top Wx - b_1^\\top x \\), where \\( b_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\), we substitute \\( b_1 \\) into \\( q_1(x) \\) to obtain \\( q_1(x) = \\frac{1}{2}x^\\top Wx - x_1 \\).\n",
        "\n",
        "The gradient \\( \\nabla q_1(x) = \\frac{1}{2}(W + W^\\top)x - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\), which, when set to zero, leads to \\( (W + W^\\top)x = 2\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\).\n",
        "\n",
        "Substituting the given \\( W \\) and solving for \\( x^* \\), we find \\( x^* = \\left[ \\frac{2}{t}, -1 \\right] \\).\n",
        "\n",
        "The minimum value of \\( q_1(x) \\) for \\( t \\neq 0 \\) is \\( \\frac{1}{t^2} - \\frac{t}{2} - \\sqrt{t} + 1 \\).\n",
        "\n",
        "# Analysis\n",
        "\n",
        "## Uniqueness of Minimizers\n",
        "Both functions \\( q_1(x) \\) and \\( q_2(x) \\) possess unique minimizers.\n",
        "\n",
        "## Local and Global Minima\n",
        "Both \\( q_1(x) \\) and \\( q_2(x) \\) exhibit convex forms. Hence, their local minima also serve as global minima.\n",
        "\n",
        "## Convexity\n",
        "- \\( q_1(x) \\) may not exhibit convexity due to the nature of \\( W \\), which is generally not symmetric.\n",
        "- \\( q_2(x) \\) demonstrates convexity since \\( A \\) is symmetric and positive definite.\n",
        "\n",
        "These analyses offer clear insights into the minimum values, uniqueness of minimizers, and convexity of \\( q_1(x) \\) and \\( q_2(x) \\).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6xAspch9-Kc"
      },
      "source": [
        "#part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import norm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1707502251.4644086 1707502251.4645755\n"
          ]
        }
      ],
      "source": [
        "s=time.time()\n",
        "for i in range(45):\n",
        "    pass\n",
        "t=time.time()\n",
        "print(s,t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f8sCmb5Sc7Qx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iterations for Accelerated Gradient Descent: 27693233\n",
            "Iterations for Gradient Descent: 55371021\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "x_0 = np.array([3, 5])\n",
        "b1 = np.array([1, 0])\n",
        "W = np.array([[0.001, 0.001**0.5],\n",
        "              [0.001**0.5, 1 + 0.001]])\n",
        "\n",
        "def f(x):\n",
        "    return 1/2 * np.matmul(np.matmul(x, W), x.T) - np.matmul(b1, x.T)\n",
        "\n",
        "def gradf(x):\n",
        "    return np.matmul(W, x.T) - b1\n",
        "\n",
        "def pertgradf(x_new, x_old, beta):\n",
        "    return gradf(x_new + beta * (x_new - x_old))\n",
        "\n",
        "alpha_k = 2 / (3 + np.sqrt(9 - 4 * 0.001**2))\n",
        "u_0 = (3 + np.sqrt(9 - 4 * 0.001**2)) / (3 - np.sqrt(9 - 4 * 0.001**2))\n",
        "B_k = (u_0**0.5 - 1) / (u_0**0.5 + 1)\n",
        "\n",
        "def accelerated_gradient_descent(x_init, tau):\n",
        "    x = x_init\n",
        "    grad = gradf(x)\n",
        "    x_neg = x_init\n",
        "    iterations = 0\n",
        "    \n",
        "    while np.linalg.norm(grad) > tau:\n",
        "        iterations += 1\n",
        "        x_neg = x\n",
        "        x_pert = x - alpha_k * pertgradf(x, x_neg, B_k)\n",
        "        x = x_pert + B_k * (x_pert - x_neg)\n",
        "        grad = gradf(x_pert)\n",
        "\n",
        "    return x, iterations\n",
        "\n",
        "# Gradient Descent with the given step size\n",
        "def gradient_descent(x_init, tau):\n",
        "    x = x_init\n",
        "    grad = gradf(x)\n",
        "    iterations = 0\n",
        "    while np.linalg.norm(grad) > tau:\n",
        "        iterations += 1\n",
        "        x = x - alpha_k * grad\n",
        "        grad = gradf(x)\n",
        "    return x, iterations\n",
        "\n",
        "def plot_error_vs_iterations(algorithm, x_init, tau):\n",
        "    x_opt, iterations = algorithm(x_init, tau)\n",
        "    return iterations\n",
        "\n",
        "tau = 1e-8\n",
        "x_init = np.array([3, 5])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "iterations_nesterov = plot_error_vs_iterations(accelerated_gradient_descent, x_init, tau)\n",
        "iterations_gradient_descent = plot_error_vs_iterations(gradient_descent, x_init, tau)\n",
        "\n",
        "print('Iterations for Accelerated Gradient Descent:', iterations_nesterov)\n",
        "print('Iterations for Gradient Descent:', iterations_gradient_descent)\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Log Error of Functional Values')\n",
        "plt.title('Log Error of Functional Values vs. Number of Iterations')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uS5qEgNA7saC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------\n",
            "Iteration =  55371021\n",
            "Minimizer =  [1000999.98998494  -31622.7762853 ]\n",
            "Final value =  -500499.9999999405\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def q1(x, W, b1):\n",
        "    return 0.5 * np.dot(np.dot(x, W), x) - np.dot(b1, x)\n",
        "\n",
        "def grad_q1(x, W, b1):\n",
        "    return np.dot(W, x) - b1\n",
        "\n",
        "def gradient_descent(x0, alpha, tau, t):\n",
        "    x = np.copy(x0)\n",
        "    W = np.array([[t, np.sqrt(t)], [np.sqrt(t), t + 1]])\n",
        "    b1 = np.array([1., 0.])\n",
        "    k = 0\n",
        "    functional_values = []\n",
        "    while np.linalg.norm(grad_q1(x, W, b1)) > tau:\n",
        "        x = x - alpha * grad_q1(x, W, b1)\n",
        "        k += 1\n",
        "        functional_values.append(q1(x, W, b1))\n",
        "\n",
        "    return k, x, q1(x, W, b1), functional_values\n",
        "\n",
        "tau = 10**(-8)\n",
        "x0 = np.array([3, 5])\n",
        "t = 0.001\n",
        "alpha = 2 / (3 + np.sqrt(9 - 4 * t**2))\n",
        "\n",
        "iteration2, minimizer2, final_value2, function_value2 = gradient_descent(x0, alpha, tau, t)\n",
        "\n",
        "print('----------------------------------------------------------------------------------')\n",
        "print('Iteration = ', iteration2)\n",
        "print('Minimizer = ', minimizer2)\n",
        "print('Final value = ', final_value2)\n",
        "print('----------------------------------------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53miM-wK-qa1"
      },
      "source": [
        "# OBSERVATION\n",
        "It is observed that employing the method of accelerated gradient descent results in fewer iterations required to reach the minimum value compared to the normal gradient descent algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpA2kmZV-vr_"
      },
      "source": [
        "#part 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9XNYpT1Z7Vu2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------\n",
            "Type = Accelerated Gradient Descent\n",
            "Iteration =  18\n",
            "Minimizer =  [0.09090909 0.63636364]\n",
            "Final value =  -0.6818181818181818\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def q2(x):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return 0.5 * x @ A @ x - b2 @ x\n",
        "\n",
        "def grad_q2(x):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return A @ x - b2\n",
        "\n",
        "def accelerated_gradient_descent(x0, alpha, beta, tolerance):\n",
        "    x = np.copy(x0)\n",
        "    x_prev = np.copy(x0)\n",
        "    k = 0\n",
        "    functional_values = []\n",
        "\n",
        "    while np.linalg.norm(grad_q2(x)) > tolerance:\n",
        "        grad_perturbed = grad_q2(x + beta * (x - x_prev))\n",
        "\n",
        "        x_next = x - alpha * grad_perturbed + beta * (x - x_prev)\n",
        "        x_prev = np.copy(x)\n",
        "        x = np.copy(x_next)\n",
        "        k += 1\n",
        "        functional_values.append(q2(x_prev))\n",
        "\n",
        "    return k, x_next, q2(x), functional_values\n",
        "\n",
        "tolerance = 1e-8\n",
        "x0 = np.array([3., 5.])\n",
        "alpha = 2 / (7 + np.sqrt(5))\n",
        "mu = (7 + np.sqrt(5)) / (7 - np.sqrt(5))\n",
        "beta = (np.sqrt(mu) - 1) / (np.sqrt(mu) + 1)\n",
        "\n",
        "iteration3, minimizer3, final_value3, function_value3 = accelerated_gradient_descent(x0, alpha, beta, tolerance)\n",
        "print('----------------------------------------------------------------------------------')\n",
        "print('Type = Accelerated Gradient Descent')\n",
        "print('Iteration = ', iteration3)\n",
        "print('Minimizer = ', minimizer3)\n",
        "print('Final value = ', final_value3)\n",
        "print('----------------------------------------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "85W2S0R0_cQw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------\n",
            "Type = Normal Gradient Descent\n",
            "Iteration =  28\n",
            "Minimizer =  [0.09090909 0.63636364]\n",
            "Final value =  -0.6818181818181819\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def grad_descent(x0, alpha, tau):\n",
        "    x = np.copy(x0)\n",
        "    k = 0\n",
        "    functional_values = []\n",
        "\n",
        "    while np.linalg.norm(grad_q2(x)) > tau:\n",
        "        x = x - alpha * grad_q2(x)\n",
        "        k += 1\n",
        "        functional_values.append(q2(x))\n",
        "\n",
        "    return k, x, q2(x), functional_values\n",
        "\n",
        "tau = 1e-8\n",
        "x0 = np.array([3, 5])\n",
        "alpha = 2 / (7 + np.sqrt(5))\n",
        "\n",
        "iteration4, minimizer4, final_value4, function_value4 = grad_descent(x0, alpha, tau)\n",
        "\n",
        "print('----------------------------------------------------------------------------------')\n",
        "print('Type = Normal Gradient Descent')\n",
        "print('Iteration = ', iteration4)\n",
        "print('Minimizer = ', minimizer4)\n",
        "print('Final value = ', final_value4)\n",
        "print('----------------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rF7hNd0_htY"
      },
      "source": [
        "# OBSERVATION\n",
        "In this scenario, it is evident that the normal gradient descent algorithm requires 10 additional iterations compared to the accelerated gradient descent method to reach the minimum value for the given function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9a6TCjk_zDf"
      },
      "source": [
        "#Part 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "16lWibRc_lHv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------\n",
            "Max no of iterations =  100\n",
            "Minimizer =  [-0.1191874   0.01564742]\n",
            "Final value = 0.383600910872348\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  500\n",
            "Minimizer =  [0.43711819 0.19075968]\n",
            "Final value = 0.003963895592601297\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  1000\n",
            "Minimizer =  [0.49185363 0.24187917]\n",
            "Final value = 6.652998985998079e-05\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  5000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value = 2.207963373434313e-18\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  10000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value = 1.262177448353619e-29\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  50000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value = 1.262177448353619e-29\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  100000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value = 1.262177448353619e-29\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  500000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value = 1.262177448353619e-29\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  1000000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value = 1.262177448353619e-29\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  5000000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value = 1.262177448353619e-29\n"
          ]
        }
      ],
      "source": [
        "def f(x):\n",
        "    return 100 * (x[1] - x[0]**2)**2 + (0.5 - x[0])**2\n",
        "\n",
        "def gradient_f(x):\n",
        "    return np.array([-400 * x[0] * (x[1] - x[0]**2) - 2 * (0.5 - x[0]), 200 * (x[1] - x[0]**2)])\n",
        "\n",
        "def projection(x0):\n",
        "    if norm(x0) <= 1:\n",
        "        return x0\n",
        "    else:\n",
        "        return x0 / norm(x0)\n",
        "\n",
        "def ada_grad(x0, max_iter, R):\n",
        "    x = np.copy(x0)\n",
        "    xs = [x0]\n",
        "\n",
        "    dividing_factor = norm(gradient_f(x))**2\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        alpha = R / np.sqrt(dividing_factor)\n",
        "        y = x - alpha * gradient_f(x)\n",
        "\n",
        "        x = projection(y)\n",
        "        dividing_factor += norm(gradient_f(x))**2\n",
        "\n",
        "        xs.append(x)\n",
        "\n",
        "    return x, f(x), xs\n",
        "\n",
        "x0 = np.array([0, 0])\n",
        "max_iter = [10**2, 500, 10**3, 5000, 10**4, 50000, 10**5, 500000, 10**6, 5000000]\n",
        "\n",
        "for t in max_iter:\n",
        "    R = 2\n",
        "    minimizer, minimum, xks = ada_grad(x0, t, R)\n",
        "    print('------------------------------------------------------')\n",
        "    print('Max no of iterations = ', t)\n",
        "    print('Minimizer = ', minimizer)\n",
        "    print('Final value =', minimum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BtIs_8YYAUme"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------\n",
            "Max no of iterations =  100\n",
            "Minimizer =  [-0.89310725 -0.4498438 ]\n",
            "Final value = -67.54634626760335\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  500\n",
            "Minimizer =  [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414145\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  1000\n",
            "Minimizer =  [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414157\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  5000\n",
            "Minimizer =  [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414151\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  10000\n",
            "Minimizer =  [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414145\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  50000\n",
            "Minimizer =  [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414157\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  100000\n",
            "Minimizer =  [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414145\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  500000\n",
            "Minimizer =  [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414145\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  1000000\n",
            "Minimizer =  [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414151\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  5000000\n",
            "Minimizer =  [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414155\n"
          ]
        }
      ],
      "source": [
        "def f2(x):\n",
        "    t = 5 * x[0] - 5\n",
        "    m = 5 * x[1] - 5\n",
        "    return np.sin(m) * np.exp((1 - np.cos(t))**2) + np.cos(t) * np.exp((1 - np.sin(m))**2) + (t - m)**2\n",
        "\n",
        "def grad_f2(x):\n",
        "    t = 5 * x[0] - 5\n",
        "    m = 5 * x[1] - 5\n",
        "    return np.array([\n",
        "        np.sin(m) * np.exp((1 - np.cos(t))**2) * 10 * np.sin(t) * (1 - np.cos(t)) - 5 * np.exp((1 - np.sin(m))**2) * np.sin(t) + 10 * (t - m),\n",
        "        np.cos(m) * np.exp((1 - np.cos(t))**2) * 5 - 10 * np.cos(t) * np.cos(m) * (1 - np.sin(m)) * np.exp((1 - np.sin(m))**2) - 10 * (t - m)\n",
        "    ])\n",
        "\n",
        "def projection(x0):\n",
        "    if norm(x0) <= 1:\n",
        "        return x0\n",
        "    else:\n",
        "        return x0 / norm(x0)\n",
        "\n",
        "def ada_grad2(x0, max_iter, R):\n",
        "    x = np.copy(x0)\n",
        "    xs = [x0]\n",
        "    dividing_factor = norm(grad_f2(x))**2\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        alpha = R / np.sqrt(dividing_factor)\n",
        "        yt = x - alpha * grad_f2(x)\n",
        "        x = projection(yt)\n",
        "        dividing_factor += norm(grad_f2(x))**2\n",
        "        xs.append(x)\n",
        "\n",
        "    return x, f2(x), xs\n",
        "\n",
        "x0 = np.array([0, 0])\n",
        "max_iter = [10**2, 500, 10**3, 5000, 10**4, 50000, 10**5, 500000, 10**6, 5000000]\n",
        "\n",
        "for t in max_iter:\n",
        "    R = 2\n",
        "    minimizer2, minimum2, xks2 = ada_grad2(x0, t, R)\n",
        "    print('------------------------------------------------------')\n",
        "    print('Max no of iterations = ', t)\n",
        "    print('Minimizer = ', minimizer2)\n",
        "    print('Final value =', minimum2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tigkcLEAcxm"
      },
      "source": [
        "#Part 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u3e4gc_ZAfGi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type = Conjugate Gradient\n",
            "Minimizer =  [0.09090909 0.63636364]\n",
            "Final value = -0.6818181818181818\n",
            "Number of iterations =  2\n",
            "----------------------------------------------------------------------------------\n",
            "Type = Normal Gradient Descent\n",
            "Iteration =  26\n",
            "Minimizer =  [0.09090909 0.63636363]\n",
            "Final value =  -0.6818181818181818\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def q2(x):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return 0.5 * x @ A @ x - b2 @ x\n",
        "\n",
        "def grad_q2(x):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return A @ x - b2\n",
        "\n",
        "def hess_q2():\n",
        "    return np.array([[4, 1], [1, 3]])\n",
        "\n",
        "def conjugate_grad(x0, tolerance):\n",
        "    r = -1 * grad_q2(x0)\n",
        "    d = np.copy(r)\n",
        "    x = np.copy(x0)\n",
        "    k = 0\n",
        "    while norm(r) > tolerance:\n",
        "        alpha = r @ r / (d @ hess_q2() @ d)\n",
        "        x = x + alpha * d\n",
        "        r_next = r - alpha * (hess_q2() @ d)\n",
        "        beta = -(r_next @ r_next) / (r @ r)\n",
        "        d = r_next - beta * d\n",
        "        r = np.copy(r_next)\n",
        "        k = k + 1\n",
        "    return x, q2(x), k\n",
        "\n",
        "def grad_descent(x0, alpha, tau):\n",
        "    x = np.copy(x0)\n",
        "    k = 0\n",
        "    while np.linalg.norm(grad_q2(x)) > tau:\n",
        "        x = x - alpha * grad_q2(x)\n",
        "        k = k + 1\n",
        "    return k, x, q2(x)\n",
        "\n",
        "tolerance = 1e-8\n",
        "x0 = np.array([5, 3])\n",
        "minimizer5, final_value5, iterations5 = conjugate_grad(x0, tolerance)\n",
        "\n",
        "alpha = 2 / (7 + np.sqrt(5))\n",
        "iteration6, minimizer6, final_value6 = grad_descent(x0, alpha, tolerance)\n",
        "\n",
        "print('Type = Conjugate Gradient')\n",
        "print('Minimizer = ', minimizer5)\n",
        "print('Final value =', final_value5)\n",
        "print('Number of iterations = ', iterations5)\n",
        "print('----------------------------------------------------------------------------------')\n",
        "print('Type = Normal Gradient Descent')\n",
        "print('Iteration = ', iteration6)\n",
        "print('Minimizer = ', minimizer6)\n",
        "print('Final value = ', final_value6)\n",
        "print('----------------------------------------------------------------------------------')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
